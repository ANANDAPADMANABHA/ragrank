(why-ragrank)=
# Why Ragrank

Having worked with ML and NLP models, we were continuosly frustated with numerous hidden failures in our models which led to us building Ragrank. 

saw release of OpenAI evals, where they proposed the use of LLMs to grade the model responses. Furthermore, we gained confidence to approach this after reading how Anthropic leverages RLAIF and dived right into the LLM evaluations research

**Key Points about Ragrank**:
- Have robust evaluations let you try out different setups and pick the best one without guesswork, which keeps your system from getting worse over time.
- Show you exactly where your system messes up, so you can figure out why and fix it before anyone else notices or stops using your product.
- Evaluations make your system transparent, which helps your users trust it more, especially if you're selling to businesses.
- They give you a clear picture of how well your system is performing, so you can make improvements and keep your users happy.
- With evaluations, you can be confident that your system is working well and giving your users the best experience possible.

----------

```{admonition} something more
:class: Tip

Before going, we have to tell you something. Our one and only mission is to build a metric driven AI system. We are doing our best to bring the best metrics and integrating with best tools we can. If you find any issues or problems please post it in the github issues tab.
```